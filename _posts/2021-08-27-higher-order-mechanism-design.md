---
layout: post
title: "Higher-order mechanism design"
date: 2021-08-29 05:23:00
categories: [thoughts]
published: false
toc: true
---

A recent medium article by Nate Coffman---[Designing Resonant Forms of Social Organization](https://ncoffman96.medium.com/designing-resonant-forms-of-social-organization-949a93bc1e97)---has prompted me to revisit a question that has been haunting this project since the beginning. Say you believe, as [I do]({{site.baseurl}}/2021/08/04/structure-agency.html), that many of the social problems we face derive from intrinsic features of the logic of interaction and communication, rather than from factors of individual psychology. This is to believe, then, that the solutions to these problems are ultimately not to be found in the domain of self-development, but in the architecture of social technologies and institutions.

<!--more-->

This spirit informs Nate's article, which snakes through a whole panoply of topics from architecture to Max Weber to virtue ethics, before making the overarching theme explicit with a closing discussion on mechanism design, or "the empirical field of engineering incentives." Now, I'm very much on board with all of this, but it does strike me that there's a crucial distinction to be made at this point which is easy to lose sight of. I've grasped at this distinction in a [few]({{site.baseurl}}/2020/08/26/subjectivation.html) [posts](({{site.baseurl}}/2021/04/13/consensus.html)) in the past, and continued to grasp at it with a hasty comment on the article, stressing a need to distinguish between 'trustless' and 'trustful' coordination mechanisms, suggesting this criss-crossed in complicated ways with 'transcendent' and 'immanent' models of authority. I thought I'd take this opportunity to clarify what I'm getting at.

## Two responses to the prisoner's dilemma

The distinction I want to make corresponds to two different approaches to the problem of cooperation posed by the prisoner's dilemma. The first seeks to manipulate the incentives until cooperation becomes an equilibrium strategy. The simplest way to ensure cooperation, as everyone knows, is to have the mob boss threaten to shoot defectors. This pattern implements a Hobbesian Leviathan---one actor with a monopoly on the use of coercive power establishes the forcing function. Of course this centralisation of power comes with its own problems, and there are various ways to achieve a similar skew without an authoritarian bottleneck: free markets, assurance contracts, Nakamoto consensus, etc. This is the domain of mechanism design proper---it's general approach is to take the decision theoretic framework as a given (i.e it is assumed that the problem is how to get game theoretic agents---rational utility-maximisers---to cooperate), then engineer the incentives to optimise for socially desirable Nash equilibria, ideally in some elegant decentralised way involving many more carrots than sticks. An important thing to note about this approach is that it doesn't actually solve the prisoner's dilemma; instead it aims to arrange things so it doesn't even come up.

The other approach is kind of the opposite: it leaves the incentives untouched, and instead considers how the decision theory itself can be modified to secure a socially desirable outcome. This is probably best illustrated by an example, of which my go-to is Douglas Hofstadter's concept of [superrationality](https://en.wikipedia.org/wiki/Superrationality). A superrational agent is basically the same as a game theoretic agent, but with the added criterion that they assume all other players are superrational and that superrational players will always pick the same strategy as each other. That definition might look a bit weird and circular, but fundamentally the idea is that a superrational agent will reason according to the Kantian categorical imperative: they will only play a strategy that would optimise outcomes when universalised across all players. The important point being that superrational players will cooperate in a prisoner's dilemma (and by this I mean an actual prisoner's dilemma, i.e. one whose incentives haven't been skewed by assurances of retribution or treats). The equilibrium strategies for superrational agents often differ from the Nash equilibria associated with the same incentives, and for this reason I'll refer to them as 'Kantian equilibria.'

## Two levels of practical deliberation

What is the significance of this distinction? To begin with, it highlights the fact that practical deliberation happens at two levels: the first involves working out how you should act given your preferences and available information, while the second level involves working out which decision procedure to use at the first level. What I shall refer to as '1st-order mechanism design' is concerned only with the first-level questions, and presupposes an answer to the second-level question. Indeed, it is only by making these modelling assumptions that it can achieve mathematical rigour: only once you have stipulated that you are dealing with game theoretic agents pre-furnished with well-defined utility functions can you get down to the no-bullshit business of designing incorruptible mechanisms with Pareto efficient Nash equilibria.

But in real world situations, the answers to the 2nd-level questions cannot be taken as given. Contrary to one of its more shallow objections, game theory does not model humans as 'selfish' in the colloquial sense---game theoretic 'self-interest' means 'utility-maximisation,' and there's no reason why my utility function can't assign high value to your well-being, or even selflessly assign it higher value than my own. But there are some modelling assumptions made by game theory which do fail to capture common patterns of practical reasoning. For instance, the 'utility function' characterisation implies outcomes are weighed by agents on a single, shared scale of value, and are therefore negotiable relative to each other, though in practice people often reason on the basis of non-negotiable values and deontological hard limits. Game theory also models individual utility functions as independent of one another, which is to say there are no utility functions which take other utility functions as inputs, even though in practice we encorporate other people's valuations into our own valuations all the time (there's a world of difference between me acting in the way I think will best benefit your well-being, and me deferring to you to tell me how to best act in the interests of your well-being). Furthermore (and most relevant to the present concerns) people commonly engage in forms of group reasoning better modelled as something like 'local superrationality' than by vanilla game theory, particularly on small and medium scales. The only point here is that empirically speaking, decision procedures are pluralistic, and so deciding which one to employ in a given situation is a non-trivial problem.

## 2nd-order mechanism design

All of which raises the question: is 2nd-order mechanism design a thing? Where 1st-order mechanism design (1-MD) tries to engineer mechanisms with socially desirable Nash equilibria, 2nd-order mechanism design (2-MD) would be concerned with designing interaction mechanisms in which players with no prior stipulated decision procedure (let's call them 'open agents') will converge on a particular decision procedure (and thus on, say, Kantian equilibria rather than Nash equilibria). Is there any reason to believe this is even possible? Well, why not---the choice of decision procedure made by an open agent is always going to be rationally sensitive to their perception of other players' decision procedure, so it can in principle be forced by engineering those perceptions. For example, an agent who reasons superrationally in a context populated by game theoretic agents is vulnerable to exploitation, so the perception that other players are reasoning game theoretically would provide an open agent with reasons _not_ to employ a superrational decision procedure. So if you wanted to _design_ an interaction mechanism that nudges open agents towards Nash equilibria rather than Kantian equilibria, you could simply fix it so that players appear to one another to be reasoning as game theoretic agents.

This is starting to get a bit complicated---so to take an example, consider the modern concept of 'professionalism.' This terms denotes a social institution consisting of many norms and taboos, ranging from obvious dos and donts (like 'don't abuse your colleagues') to a much more subtle web of soft expectations and potential _faux pas_. When it comes to pay negotiation, for instance, it is tacitly expected that employees advocate for their own interests in securing the best deal they can from their employer in private, and indeed possessing the assertiveness required to do so is seen as a mark of professional seriousness. On the other hand, it is seen as bad form to openly discuss pay with your colleagues, and is liable to get you marked as unionising troublemaker. To work in a professional context is to know that others are aware of these norms, that they are aware that you are aware of them, and so on---in short, it is to occupy a social space in which you expect everyone to deliberate as game theoretic agents, which makes it unwise to not do so yourself. It is not that everyone actually _is_ a game theoretic agent in any deep sense (real people are more akin to open agents), but that the social norms of their interaction context actively _materialise_ this reality by shaping everyone's expectations.

And of course, what is discouraged by these norms is the formation of groups which have far greater bargaining power than individuals would have alone---i.e. into blocks of locally superrational agents who could leverage a cooperative Kantian equilibrium to their mutual advantage. In practice this is achieved by disincentivising certain kinds of peer-to-peer  communication with one hand while encouraging vertical negotiation with the other.

## The double life of incentives

This example shines a light on a certain ambiguity, concerning the role played by incentives in driving behaviours. On the one hand incentives play their familiar 1-MD role: the shape of the incentive structure defines the Nash equilibria of an interaction between game theoretic agents. But it appears they can also play a 2-MD role: incentives allow open agents to reason about which decision procedure to use in the first place.

Given any real-world interaction mechanism, then, we might read the incentive structure in two different ways. In the example above professional norms were read in a 2-MD sense, as materialising a game theoretic (rather than superrational) structure of agency. But they could also be read in the 1-MD way: in the labour market, the socially desirable outcome is when the pricing of a individual's labour provides an accurate reflection of their value, so everyone can make informed decision about how to allocate resources in the most efficient way. When people bypass the market to create unions which negotiate as a single entity, they distort the accuracy of this information, undermining the pricing system's ability to communicate differences between individual workers, introducing inefficiencies which are ultimately to the detriment of everyone. The institution of professionalism is simply a _good mechanism_ for achieving the most socially desirable Nash equilibrium.

But of course, this argument depends on the assumption that what we were aiming at was the best _Nash_ equilibrium, which in turn depends on the presupposition that all there is to work with in the first place is game theoretic agents. If workers end up in peer-to-peer prisoner's dilemmas which drive wages to rock bottom, then this is still the optimal situation given these presuppositions. Indeed, anyone who clubs together is interpreted as seeking a competitive edge in a manner guaranteed to produce suboptimal equilibrium, so incentives are engineered to phase this behaviour out. But from the 2-MD perspective, these presuppositions are not really givens---not something that existed prior to the introduction of the mechanism---but are in fact produced by it. If the mechanism materialises its own presuppositions, then it is hardly surprising that it looks successful by its own lights. But the real question is, what has been given up?  What if you have bought an improved Nash equilibrium at the price of making Kantian equilibrium impossible? Taken alone, the 1-MD perspective is blind to this kind of trade-off.

## Trust ≠ centralisation

Say this _had_ happened: your 1-MD was also functioning tacitly as a piece of 2-MD, and that your optimal Nash equilibrium came at the price of blocking a Kantian equilibrium. What would be the problem with this? What is so great about Kantian equilibria anyway? Well, in one sense this has already been answered: superrationality actually solves the prisoner's dilemma, whereas engineering the Nash equilibria can only avoid, defer or displace it. But who cares? If you can successfully avoid it, then the question of whether you can solve it is surely moot.





<!-- ## References
{% bibliography --cited %} -->
