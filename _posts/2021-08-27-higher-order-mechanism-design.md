---
layout: post
title: "Higher-order mechanism design"
date: 2021-08-29 05:23:00
categories: [thoughts]
published: false
---

A recent medium article by Nate Coffman---[Designing Resonant Forms of Social Organization](https://ncoffman96.medium.com/designing-resonant-forms-of-social-organization-949a93bc1e97)---has prompted me to revisit a question that has been haunting this project since the beginning. Say you believe, as [I do]({{site.baseurl}}/2021/08/04/structure-agency.html), that many of the social problems we face derive from intrinsic features of the logic of interaction and communication, rather than from factors of individual psychology. This is to believe, then, that the solutions to these problems are ultimately not to be found in the domain of self-development, but in the architecture of social technologies and institutions.

This spirit informs Nate's article, which snakes through a whole panoply of topics from architecture to Max Weber to virtue ethics, before making the overarching theme explicit with a closing discussion on mechanism design, or "the empirical field of engineering incentives." Now, I'm very much on board with all of this, but it does strike me that there's a crucial distinction to be made at this point which is easy to lose sight of. I've grasped at this distinction in a [few]({{site.baseurl}}/2020/08/26/subjectivation.html) [posts](({{site.baseurl}}/2021/04/13/consensus.html)) in the past, and continued to grasp at it with a hasty comment on the article, stressing a need to distinguish between 'trustless' and 'trustful' coordination mechanisms, suggesting this criss-crossed in complicated ways with 'transcendent' and 'immanent' models of authority. I thought I'd take this opportunity to clarify what I'm getting at.

## Two responses to the prisoner's dilemma

The distinction I want to make corresponds to two different approaches to the problem of cooperation posed by the prisoner's dilemma. The first seeks to manipulate the incentives until cooperation becomes an equilibrium strategy. The simplest way to ensure cooperation, as everyone knows, is to have the mob boss threaten to shoot defectors. This pattern implements a Hobbesian Leviathan---one actor with a monopoly on the use of coercive power establishes the forcing function. Of course this centralisation of power comes with its own problems, and there are various ways to achieve a similar skew without an authoritarian bottleneck: free markets, assurance contracts, Nakamoto consensus, etc. This is the domain of mechanism design proper---it's general approach is to take the decision theoretic framework as a given (i.e it is assumed that the problem is how to get game theoretic agents---rational utility-maximisers---to cooperate), then engineer the incentives to optimise for socially desirable Nash equilibria, ideally in some elegant decentralised way involving many more carrots than sticks. An important thing to note about this approach is that it doesn't actually solve the prisoner's dilemma; instead it aims to arrange things so it doesn't even come up.

The other approach is kind of the opposite: it leaves the incentives untouched, and instead considers how the decision theory itself can be modified to secure a socially desirable outcome. This is probably best illustrated by an example, of which my go-to is Douglas Hofstadter's concept of [superrationality](https://en.wikipedia.org/wiki/Superrationality). A superrational agent is basically the same as a game theoretic agent, but with the added criterion that they assume all other players are superrational and that superrational players will always pick the same strategy as each other. That definition might look a bit weird and circular, but fundamentally the idea is that a superrational agent will reason according to the Kantian categorical imperative: they will only play a strategy that would optimise outcomes when universalised across all players. The important point being that superrational players will cooperate in a prisoner's dilemma (and by this I mean an actual prisoner's dilemma, i.e. one whose incentives haven't been skewed by assurances of retribution or treats). The equilibrium strategies for superrational agents often differ from the Nash equilibria associated with the same incentives, and for this reason I'll refer to them as 'Kantian equilibria.'

## Two levels of practical deliberation

What is the significance of this distinction? To begin with, it highlights the fact that practical deliberation happens at two levels: the first involves working out how you should act given your preferences and available information, while the second level involves working out which decision procedure to use at the first level. What I shall refer to as '1st-order mechanism design' is concerned only with the first-level questions, and assumes answers to second-level questions as given. Indeed, it is only by making these modelling assumptions that it can achieve mathematical rigour: only once you have stipulated that you are dealing with game theoretic agents pre-furnished with well-defined utility functions can you get down to the no-bullshit business of designing incorruptible mechanisms with Pareto efficient Nash equilibria.

But in real world situations, the answers to the 2nd-level questions cannot be taken as given. Contrary to one of its more shallow objections, game theory does not model humans as 'selfish' in the colloquial sense---game theoretic 'self-interest' means 'utility-maximisation,' and there's no reason why my utility function can't assign high value to your well-being, or even selflessly assign it higher value than my own. But there are some modelling assumptions made by game theory which do fail to capture common patterns of practical reasoning. For instance, the 'utility function' characterisation implies outcomes are weighed by agents on a single, shared scale of value, and are therefore negotiable relative to each other, though in practice people often reason on the basis non-negotiable values and deontological hard limits. Game theory also models individual utility functions as independent of one another, which is to say there are no utility functions which take other utility functions as inputs, even though in practice we encorporate other people's valuations into our own valuations all the time (there's a world of difference between me acting in the way I think will best benefit your well-being, and me deferring to you to tell me how to best act in the interests of your well-being). Furthermore (and most relevant to the present concerns) people commonly engage in forms of group reasoning better modelled as something like 'local superrationality' than by vanilla game theory, particularly on small and medium scales. The only point here is that empirically speaking, reasoning practices are pluralistic, and so deciding which one to employ in a given situation is a non-trivial problem.

## 2nd-order mechanism design

All of which raises the question: is 2nd-order mechanism design a thing? Where 1st-order mechanism design (1-MD) tries to engineer mechanisms with socially desirable Nash equilibria, 2nd-order mechanism design (2-MD) would be concerned with designing interaction mechanisms in which as-yet undetermined actors (let's call them 'open agents') will converge on a particular decision procedure (and thus on, say, Kantian equilibria rather than Nash equilibria). Is there any reason to believe this is even possible? Well, why not---the choice of decision procedure made by an open agent is always going to be rationally sensitive to their perception of other players' decision procedure, so it can in principle be forced by engineering those perceptions. For example, an agent who reasons superrationally in a context populated by game theoretic agents is vulnerable to exploitation, so the perception that other players are reasoning game theoretically would provide an open agent with reasons _not_ to employ a superrational decision procedure. So if you wanted to _design_ an interaction mechanism that nudges open agents towards Nash equilibria as opposed to Kantian equilibria, you could design one in which all players appeared to one another to be reasoning as if they were game theoretic agents.



<!-- ## References
{% bibliography --cited %} -->
