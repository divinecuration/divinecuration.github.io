---
layout: post
title: "Higher-order mechanism design"
date: 2021-08-23 05:23:00
categories: [thoughts]
published: false
---

A recent medium article by Nate Coffman---[Designing Resonant Forms of Social Organization](https://ncoffman96.medium.com/designing-resonant-forms-of-social-organization-949a93bc1e97)---has prompted me to revisit a question that has been haunting this project since the beginning. Say you believe, as [I do]({{site.baseurl}}/2021/08/04/structure-agency.html), that many of the social problems we face derive from intrinsic features of the logic of interaction and communication, rather than from factors of individual psychology. This is to believe, then, that the solutions to these problems are ultimately not to be found in the domain of self-development, but in the architecture of social technologies and institutions.

This spirit informs Nate's article, which snakes through a whole panoply of topics from architecture to Max Weber to virtue ethics, before making the overarching theme explicit with a closing discussion on mechanism design, or "the empirical field of engineering incentives." Now, I'm very much on board with all of this, but it does strike me that there's a crucial distinction to be made at this point which is easy to lose sight of. I've grasped at this distinction in a [few]({{site.baseurl}}/2020/08/26/subjectivation.html) [posts](({{site.baseurl}}/2021/04/13/consensus.html)) in the past, and continued to grasp at it with a hasty comment on the article, stressing a need to distinguish between 'trustless' and 'trustful' coordination mechanisms, suggesting this criss-crossed in complicated ways with 'transcendent' and 'immanent' models of authority. I thought I'd take this opportunity to clarify what I'm getting at.

The distinction I want to make corresponds to two different approaches to the problem of cooperation posed by the prisoner's dilemma. The first seeks to manipulate the incentives until cooperation becomes an equilibrium strategy. The simplest way to ensure cooperation, as everyone knows, is to have the mob boss threaten to shoot defectors. This pattern implements a Hobbesian Leviathan---one actor with a monopoly on the use of coercive power establishes the forcing function. Of course this centralisation of power comes with its own problems, and there are various ways to achieve a similar skew without an authoritarian bottleneck: free markets, assurance contracts, Nakamoto consensus, etc. This is the domain of mechanism design proper---it's general approach is to take the decision theoretic framework as a given (i.e it is assumed that the problem is how to get game theoretic agents---rational utility-maximisers---to cooperate), then engineer the incentives to optimise for socially desirable Nash equilibria, ideally in some elegant decentralised way involving many more carrots than sticks. An important thing to note about this approach is that it doesn't actually solve the prisoner's dilemma; instead it aims to arrange things so it doesn't even come up.

The other approach is kind of the opposite: it leaves the incentives untouched, and instead considers how the decision theory itself can be modified to secure a socially desirable outcome. This is probably best illustrated by an example, of which my go-to is Douglas Hofstadter's concept of [superrationality](https://en.wikipedia.org/wiki/Superrationality). A superrational agent is basically the same as a game theoretic agent, but with the added criterion that they assume all other players are superrational and that superrational players will always pick the same strategy as each other. That definition might look a bit weird and circular, but fundamentally the idea is that a superrational agent will reason according to the Kantian categorical imperative: they will only play a strategy that would optimise outcomes when universalised across all players. The important point being that superrational players will cooperate in a prisoner's dilemma (and by this I mean an actual prisoner's dilemma, i.e. one whose incentives haven't been skewed by assurances of retribution or treats). The equilibrium strategies for superrational agents often differ from the Nash equilibria associated with the same incentives, and for this reason I'll refer to them as 'Kantian equilibria.'

What is the significance of this distinction? To begin with, it highlights the fact that practical deliberation happens at two levels: the first involves working out how you should act given your preferences and available information, while the second level involves working out which decision procedure to use at the first level. What I shall refer to as '1st-order mechanism design' is concerned only with the first-level questions, and assumes answers to second-level questions as given. Indeed, it is only by making these modelling assumptions that it can achieve mathematical rigour: only once you have stipulated that you are dealing with game theoretic agents adorned with off-the-shelf utility functions can you get down to the no-bullshit business of designing trustless (and therefore ungameable) mechanisms with Pareto efficient Nash equilibria.

But in real world situations, the answers to the 2nd-level questions cannot be taken as given. Contrary to one of its more shallow objections, game theory does not model humans as 'selfish' actors in the colloquial sense---game theoretic 'self-interest' means 'utility-maximisation,' and there's no reason why my utility function can't assign high value to your well-being, or even selflessly assign it higher value than my own. But there are some modelling assumptions made by game theory which do fail to capture common patterns of practical reasoning. The 'utility' characterisation implies outcomes are weighed by agents on a single, shared scale of value, and are therefore negotiable relative to each other, though in practice people often reason on the basis non-negotiable values. Game theory also models individual utility functions as independent of one another, which is to say there are no utility functions which take other utility functions as inputs, even though in practice we do this all the time (there's a world of difference between me acting in the way I think will best benefit your well-being, and me deferring to you to tell me how to best act in the interests of your well-being). Furthermore it's extremely common---particularly on small and medium scales---for people to engage in group reasoning which is much better modelled as a kind of 'local superrationality' than by vanilla game theory. Point being that empirically speaking, practical reasoning practices are pluralistic, and deciding which one to employ in a given situation is a non-trivial problem.

All of which raises the question: is 2nd-order mechanism design a thing? Where 1st-order mechanism design (1-MD) tries to engineer mechanisms with social desirable Nash equilibria, or which will facilitate movement from an undesirable Nash equilibrium to a more desirable one, 2nd-order mechanism design (2-MD) would be concerned with designing interaction mechanisms in which as-yet undetermined actors will converge on Kantian equilibria rather than Nash equilibria.



<!-- ## References
{% bibliography --cited %} -->
