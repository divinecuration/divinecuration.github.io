---
layout: post
title: "Indeterminacy and Norm-Grounding in Brandom"
date: 2020-11-21 05:23:00
categories: [thoughts, brandom]
---

I recently came across a post by David Roden called [Robert Brandom and the Posthuman](https://enemyindustry.wordpress.com/2014/08/23/robert-brandom-and-posthumanism/) which gives, among other things, an excellent introduction to a criticism that can be levelled against Brandom's philosophical project. In this post I want to consider the nature of the threat this problem poses to Brandom, and suggest that aspects of his recent exposition of Hegel can be read as an attempt to respond to it<sup>[1](#r1)</sup>.

The dizzyingly intricate edifice built by Brandom in his earlier work, in particular the 800 page tome Making It Explicit, provides a framework within which (semantic) intentional content is ultimately understood in terms of (pragmatic) normative attitudes. If successful, this would reduce talk of the objective representational content of linguistic (or mental) items to talk of the subjective attitudes of language users. The potential cash value of this is huge: it promises no less than to give a fully naturalised account of intentional content.

Brandom himself has pointed out that the success of this project depends critically on the ontological status of normative attitudes. In particular, if normative attitudes can themselves be understood only by appeal to their _intentional_ content, then the chain of explanation is circular, and the whole thing implodes. The task for Brandom, then, is to convince us that normative attitudes can be individuated without making a tacit appeal to their intentional content. This is the problem of how social practices specified in purely behavioural or dispositional terms can be properly spoken of as materialising implicit norms.

This is where the problems start. Because it seems that no amount of behavioural or dispositional criteria can ever be sufficient to specify a determinate norm. For example regularism, which claims that norms are behavioural _regularities_, is typically rejected on account of the fact that any finite string of performances is consistent with infinitely many distinct norms. (This is referred to as the gerrymandering objection, whose canonical expression is found in Saul Kripke's _Wittgenstein on Rules and Private Language_, which [I've written a bit about previously](2020/08/18/rule-following.html).) Simple regularism can be bolstered via an appeal to dispositions---by specifying not just what does happen, but what _would_ happen under counterfactually variable conditions (call these _subjunctively robust regularities_). But this does not solve the problem, as it fails to specify conditions under which the norm is violated.

Brandom's own purported solution attempts a further bolstering by incorporating sanctioning behaviours, claiming that social practices can be properly said to contain implicit norms when deviation from the regularity provokes sanctioning behaviours from others (these could now be said to pick out _subjunctively robust self-correcting regularities_). However, others have replied that this too will succumb to the gerrymandering objection. It looks like there is simply _no_ amount of dispositional or behavioural criteria that can fully pin down the content of a norm. In order to attribute a determinate norm to a social practice then, it seems we are left with choosing between either adopting some kind of Platonism about norms (which Brandom's whole project is an attempt to avoid) or confessing that the content of the norm will always be relative to the background practices of the attributer.

Roden argues that if forced into this second option Brandom's project also fails, not because it would be wrong, per se, but because it would be rendered unilluminating. However, it seems to me there's another possible path open to Brandom at this point: he could accept that while no amount of behavioural-dispositional criteria will ever yield fully determinate norms, they may yield norms that are determinate _enough_ to ground attributions of intentional content. This would of course mean admitting a certain amount of indeterminacy into norms themselves, which would then propagate through the whole intentional nexus. But he could point to the fact that while the successive bolsterings of our naturalistically legit norm-grounding criteria---from regularities, to subjunctively robust regularities, to subjunctively robust self-correcting regularities---will never converge on a unique determinate norm, each step does represent a significant diminishment of _indeterminacy_. It could perhaps be suggested, then, that at some point this always incomplete exorcism of indeterminacy will nevertheless cross a threshold, beyond which enough friction will be present for intentions-talk to hook on to without collapsing into arbitrariness.

This strategy would be similar to that taken by Daniel Dennett in his widely discussed [Real Patterns]({{site.baseurl}}/assets/pdf/dennett-real-patterns.pdf). In this paper he argues that notions of information compression allow us to understand determinacy as a matter of degree, rather than as a binary. Using this to stake out a semi-realist position between hard realism and anti-realism, Dennett aims to avoid both the philosophical tangles of the former<sup>[2](#r2)</sup> and the collapse into arbitrariness threatened by the latter. But while Dennett seems quite happy to let some indeterminacy run loose in his semantic house, Brandom doesn't seem the sort of philosopher who would be satisfied in knowing only that a bit of indeterminacy is not a bug. One suspects he would want to show that it's a feature, too.

It's only occurred to me recently that the exposition of Hegel that Brandom develops in his most recent work can be read as attempting to do exactly this. Recollective rationality---as required for the recognitive practices constitutive of historical self-conscious---actually _leverages_ such indeterminacy.

> Exercising this kind of rationality is retrospectively rationally reconstructing the various applications of a concept, selecting a trajectory through the actual uses that picks out a sequence that is _expressively progressive_. That is one that has the form of the gradual, cumulative revelation, the emergence into ever-greater explicitness, of the contours of a determinately contentful norm that is seen to have been implicit all along.<sup>[3](#r3)</sup>

Elsewhere, Brandom gives the example of the application of the law by a judge. The responsibility of the judge is to apply the law---in this sense the law itself is authoratitive---but the judge has the authority to interpret the law with regard to particular applications. This authority is constrained: in applying the law, they must do so in a way that articulates its content such that it coheres with applications by judges in the past (viz. precedent setting). This articulation is an exercise of recollective rationality, of bringing the implicit content of the law to greater explicitness by projecting an interpretation across its applications that incorporates them all (or in certain cases may render specific past applications unlawful).

(Another example I've found useful to think about is how this account of recollective rationality could modify the kind of hard Kantian moral position that is criticised on the grounds that it entails one should not lie to a murderer to save someone's life. To do so, the Kantian argues, is to act under a norm---that "one can sometimes lie"---that does not universalise, i.e. that violates the categorical imperative. However if one accepted that the norm "one should never lie" could be revised to include mitigating circumstances in such a way that it did universalise---perhaps "one should never lie except to save someone's life"---then one could say the lie was morally justified, insofar as it was _this_ norm that was acted under. If, however, it is the case that there is _no_ determinate norm that is implicit in an act, due to gerrymandering-like objections, then establishing the moral rightness of the act becomes a question of _articulating_ the content of the implicit norm in such a way that this content both makes sense of why this act was justified, and why previous lies should be condemned---that is, through an exercise of recollective rationality.)

Understanding the norms implicit in social practices as neither completely indeterminate (in which case they wouldn't be norms) nor as fully determinate (in which case there would be no question of the gradual process of the articulation of their content), seems supported by the kind of subjunctively robust self-correcting regularities picture drawn by Brandom. Yes gerrymandering is still possible, but since gerrymandering is highly constrained we can still talk of those practices as norm-governed, and since gerrymandering is still possible within those constraints we can talk of the gradual determination of the norm over time as the practice undergoes the tribunal of experience<sup>[4](#r4)</sup>.

The semi-determinacy of norms plays out in the symmetry of the authority structure of social relations---of, say, the relationship between judges before the law. That they are judges before the _same_ law represents the relative determinacy of the norms materialised in juridical practice. That this practice involves a symmetrical attribution of authority---that judges have the same authority with respect to one another with regard to interpreting and applying the law---represents the remaining (and never fully eliminable) indeterminacy. It is this symmetrical authority structure, or "dyadic reciprocal recognition", that according to Brandom forms the basis of Hegel's historically constituted self-consciousness. The play of determinacy and indeterminacy is essential to this picture. Where Dennett's semi-realism is sometimes accused of being contrived for no other reason than to dodge a trap, within the context of Brandom's Hegelian pragmatism a similar idea can be seen to form an essential positive component of its account of rational agency.

<br />
<br />

_Notes_

1. <a name="r1"></a>This post belongs to a wider train of thought I've been following concerning the productive role of indeterminacy in systems of meaning, as encountered in e.g. Derrida's trace, Baudrillard's symbolic exchange, or even Badiou's appeal to formal undecidability in mathematical structures.

2. <a name="r2"></a>See e.g. Jerry Fodor's [mobilisation of the disjunction problem against Darwinism]({{site.baseurl}}/assets/pdf/fodor-darwin.pdf).

3. <a name="r3"></a>Robert Brandom, _A Spirit of Trust: A Reading of Hegel's Phenomenology_. p17.

4. <a name="r4"></a>Semi-determinate norms gradually articulated through the process of their application provides a way of thinking about a process being norm-governed without being goal-oriented. As such this touches on themes very much present in Deleuze & Guattari's machinic pragmatics, a nice discussion of which can be found in Ray Brassier, [_Concrete Rules and Abstract Machines: Form and Function in A Thousand Plateaus_]({{site.baseurl}}/assets/pdf/brassier-concrete-rules.pdf).
